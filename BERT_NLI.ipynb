{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "Dvc0nF1-VjMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Torchtext==0.04"
      ],
      "metadata": {
        "id": "l9jDwt8UPlQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "LIMWeJIGM3cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbscTf76VVIC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import transformers\n",
        "from torchtext import data\n",
        "from transformers import BertModel\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from transformers import *\n",
        "import math\n",
        "import time\n",
        "import torchtext\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT initial setup\n",
        "Get default tokens with their IDs"
      ],
      "metadata": {
        "id": "6HMt5fmbZpEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "cls = bert_tokenizer.cls_token\n",
        "sep = bert_tokenizer.sep_token\n",
        "pad = bert_tokenizer.pad_token\n",
        "unk = bert_tokenizer.unk_token\n",
        "\n",
        "cls_id = bert_tokenizer.cls_token_id\n",
        "sep_id = bert_tokenizer.sep_token_id\n",
        "pad_id = bert_tokenizer.pad_token_id\n",
        "unk_id = bert_tokenizer.unk_token_id\n",
        "print(cls_id,sep_id,pad_id,unk_id)"
      ],
      "metadata": {
        "id": "YUBsAXsuZoR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset setup -\n",
        "Download and extracting dataset"
      ],
      "metadata": {
        "id": "xWmwQt0LW9pU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip\n",
        "\n",
        "dataset_multinli_train = load_dataset('multi_nli',split='train') #split into train and val\n",
        "dataset_multinli_test = load_dataset('multi_nli',split='validation_matched') #test\n",
        "\n",
        "dataset_anli_train = load_dataset('anli',split='train_r2')\n",
        "dataset_anli_dev = load_dataset('anli',split='dev_r2')\n",
        "dataset_anli_test = load_dataset('anli',split='test_r2')\n"
      ],
      "metadata": {
        "id": "o-5QQKvMXEc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1 :** Loading dataset in dataframes"
      ],
      "metadata": {
        "id": "RJWTlC7RXvnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Read files\n",
        "df_train_snli = pd.read_csv('snli_1.0/snli_1.0_train.txt', sep='\\t')\n",
        "df_dev_snli = pd.read_csv('snli_1.0/snli_1.0_dev.txt', sep='\\t')\n",
        "df_test_snli = pd.read_csv('snli_1.0/snli_1.0_test.txt', sep='\\t')\n",
        "\n",
        "df_train_snli = df_train_snli[['gold_label','sentence1','sentence2']].copy()\n",
        "df_dev_snli = df_dev_snli[['gold_label','sentence1','sentence2']].copy()\n",
        "df_test_snli = df_test_snli[['gold_label','sentence1','sentence2']].copy()\n",
        "\n",
        "#Rename column names\n",
        "df_train_snli.rename(columns = {'sentence1':'premise','sentence2':'hypothesis'}, inplace = True)\n",
        "df_dev_snli.rename(columns = {'sentence1':'premise','sentence2':'hypothesis'}, inplace = True)\n",
        "df_test_snli.rename(columns = {'sentence1':'premise','sentence2':'hypothesis'}, inplace = True)\n",
        "\n",
        "# Remove rows with only one sentence\n",
        "df_train_snli.dropna(inplace=True)\n",
        "\n",
        "print(df_train_snli.head(2))\n",
        "\n",
        "\n",
        "df_train_multinli = pd.DataFrame()\n",
        "df_train_multinli['hypothesis'] = dataset_multinli_train['hypothesis']\n",
        "df_train_multinli['premise'] = dataset_multinli_train['premise']\n",
        "df_train_multinli['gold_label'] = dataset_multinli_train['label']\n",
        "df_train_multinli.dropna(inplace=True)\n",
        "df_train_multinli['gold_label'] = df_train_multinli['gold_label'].replace({0: 'neutral', 1: 'contradiction', 2: 'entailment'})\n",
        "\n",
        "df_test_multinli = pd.DataFrame()\n",
        "df_test_multinli['hypothesis'] = dataset_multinli_test['hypothesis']\n",
        "df_test_multinli['premise'] = dataset_multinli_test['premise']\n",
        "df_test_multinli['gold_label'] = dataset_multinli_test['label']\n",
        "df_test_multinli.dropna(inplace=True)\n",
        "df_test_multinli['gold_label'] = df_test_multinli['gold_label'].replace({0: 'neutral', 1: 'contradiction', 2: 'entailment'})\n",
        "\n",
        "\n",
        "df_dev_multinli = df_train_multinli[len(df_train_multinli)-50000:]\n",
        "df_train_multinli = df_train_multinli[:len(df_train_multinli)-50000]\n",
        "\n",
        "print(df_train_multinli.head(2))\n",
        "print(len(df_dev_multinli))\n",
        "print(len(df_train_multinli))\n",
        "print(len(df_test_multinli))\n",
        "\n",
        "df_train_anli = pd.DataFrame()\n",
        "df_train_anli['hypothesis'] = dataset_anli_train['hypothesis']\n",
        "df_train_anli['premise'] = dataset_anli_train['premise']\n",
        "df_train_anli['gold_label'] = dataset_anli_train['label']\n",
        "df_train_anli.dropna(inplace=True)\n",
        "df_train_anli['gold_label'] = df_train_anli['gold_label'].replace({0: 'neutral', 1: 'contradiction', 2: 'entailment'})\n",
        "\n",
        "df_dev_anli = pd.DataFrame()\n",
        "df_dev_anli['hypothesis'] = dataset_anli_dev['hypothesis']\n",
        "df_dev_anli['premise'] = dataset_anli_dev['premise']\n",
        "df_dev_anli['gold_label'] = dataset_anli_dev['label']\n",
        "df_dev_anli.dropna(inplace=True)\n",
        "df_dev_anli['gold_label'] = df_dev_anli['gold_label'].replace({0: 'neutral', 1: 'contradiction', 2: 'entailment'})\n",
        "\n",
        "df_test_anli = pd.DataFrame()\n",
        "df_test_anli['hypothesis'] = dataset_anli_test['hypothesis']\n",
        "df_test_anli['premise'] = dataset_anli_test['premise']\n",
        "df_test_anli['gold_label'] = dataset_anli_test['label']\n",
        "df_test_anli.dropna(inplace=True)\n",
        "df_test_anli['gold_label'] = df_test_anli['gold_label'].replace({0: 'neutral', 1: 'contradiction', 2: 'entailment'})\n"
      ],
      "metadata": {
        "id": "5NNg-i_IXt6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2 :** Preporcessing - limiting sentence length, adding token type and attention masks"
      ],
      "metadata": {
        "id": "8qJvu8NYX6UC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper functions for pre-processing\n",
        "def limit_sentence_length(sentence):\n",
        "  sentence = sentence.split(\" \")\n",
        "  sentence = sentence[:128]\n",
        "  return \" \".join(sentence)\n",
        "\n",
        "def get_token_type(sentence,num):\n",
        "  return [num]*len(sentence)\n"
      ],
      "metadata": {
        "id": "8NIHqLXVYLpl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df_train,df_dev,df_test):\n",
        "    #Limiting length\n",
        "    df_train['premise_trim'] = df_train['premise'].apply(limit_sentence_length)\n",
        "    df_train['hypothesis_trim'] = df_train['hypothesis'].apply(limit_sentence_length)\n",
        "\n",
        "    df_dev['premise_trim'] = df_dev['premise'].apply(limit_sentence_length)\n",
        "    df_dev['hypothesis_trim'] = df_dev['hypothesis'].apply(limit_sentence_length)\n",
        "\n",
        "    df_test['premise_trim'] = df_test['premise'].apply(limit_sentence_length)\n",
        "    df_test['hypothesis_trim'] = df_test['hypothesis'].apply(limit_sentence_length)\n",
        "\n",
        "    #Adding starting and ending token\n",
        "    df_train['premise_sos_eos'] = cls + ' ' + df_train['premise_trim'] + ' ' + sep + ' '\n",
        "    df_train['hypothesis_sos_eos'] = df_train['hypothesis_trim'] + ' ' + sep\n",
        "\n",
        "    df_dev['premise_sos_eos'] = cls + ' ' + df_dev['premise_trim'] + ' ' + sep + ' '\n",
        "    df_dev['hypothesis_sos_eos'] = df_dev['hypothesis_trim'] + ' ' + sep\n",
        "\n",
        "    df_test['premise_sos_eos'] = cls + ' ' + df_test['premise_trim'] + ' ' + sep + ' '\n",
        "    df_test['hypothesis_sos_eos'] = df_test['hypothesis_trim'] + ' ' + sep\n",
        "\n",
        "    # Tokenize\n",
        "    df_train['premise_tokens'] = df_train['premise_sos_eos'].apply(bert_tokenizer.tokenize)\n",
        "    df_train['hypothesis_tokens'] = df_train['hypothesis_sos_eos'].apply(bert_tokenizer.tokenize)\n",
        "\n",
        "    df_dev['premise_tokens'] = df_dev['premise_sos_eos'].apply(bert_tokenizer.tokenize)\n",
        "    df_dev['hypothesis_tokens'] = df_dev['hypothesis_sos_eos'].apply(bert_tokenizer.tokenize)\n",
        "\n",
        "    df_test['premise_tokens'] = df_test['premise_sos_eos'].apply(bert_tokenizer.tokenize)\n",
        "    df_test['hypothesis_tokens'] = df_test['hypothesis_sos_eos'].apply(bert_tokenizer.tokenize)\n",
        "\n",
        "    #Added token types\n",
        "    df_train['premise_token_type'] = df_train['premise_tokens'].apply(get_token_type,args={0})\n",
        "    df_train['hypothesis_token_type'] = df_train['hypothesis_tokens'].apply(get_token_type,args={1})\n",
        "\n",
        "    df_dev['premise_token_type'] = df_dev['premise_tokens'].apply(get_token_type,args={0})\n",
        "    df_dev['hypothesis_token_type'] = df_dev['hypothesis_tokens'].apply(get_token_type,args={1})\n",
        "\n",
        "    df_test['premise_token_type'] = df_test['premise_tokens'].apply(get_token_type,args={0})\n",
        "    df_test['hypothesis_token_type'] = df_test['hypothesis_tokens'].apply(get_token_type,args={1})\n",
        "\n",
        "    # Combine sentences\n",
        "    df_train['combined_sentences'] = df_train['premise_tokens'] + df_train['hypothesis_tokens']\n",
        "    df_dev['combined_sentences'] = df_dev['premise_tokens'] + df_dev['hypothesis_tokens']\n",
        "    df_test['combined_sentences'] = df_test['premise_tokens'] + df_test['hypothesis_tokens']\n",
        "\n",
        "    df_train['attention_mask'] = df_train['combined_sentences'].apply(get_token_type,args={1})\n",
        "    df_dev['attention_mask'] = df_dev['combined_sentences'].apply(get_token_type,args={1})\n",
        "    df_test['attention_mask'] = df_test['combined_sentences'].apply(get_token_type,args={1})\n",
        "\n",
        "    df_train['token_type'] = df_train['premise_token_type'] + df_train['hypothesis_token_type']\n",
        "    df_dev['token_type'] = df_dev['premise_token_type'] + df_dev['hypothesis_token_type']\n",
        "    df_test['token_type'] = df_test['premise_token_type'] + df_test['hypothesis_token_type']\n",
        "\n",
        "    # Get masks and token type of combined sentences\n",
        "    df_train['combined_sentences'] = df_train['combined_sentences'].apply(lambda x:\" \".join(x))\n",
        "    df_dev['combined_sentences'] = df_dev['combined_sentences'].apply(lambda x:\" \".join(x))\n",
        "    df_test['combined_sentences'] = df_test['combined_sentences'].apply(lambda x:\" \".join(x))\n",
        "\n",
        "    df_train['attention_mask'] = df_train['attention_mask'].apply(lambda x : \" \".join([str(y) for y in x]))\n",
        "    df_dev['attention_mask'] = df_dev['attention_mask'].apply(lambda x : \" \".join([str(y) for y in x]))\n",
        "    df_test['attention_mask'] = df_test['attention_mask'].apply(lambda x : \" \".join([str(y) for y in x]))\n",
        "\n",
        "    df_train['token_type'] = df_train['token_type'].apply(lambda x : \" \".join([str(y) for y in x]))\n",
        "    df_dev['token_type'] = df_dev['token_type'].apply(lambda x : \" \".join([str(y) for y in x]))\n",
        "    df_test['token_type'] = df_test['token_type'].apply(lambda x : \" \".join([str(y) for y in x]))\n",
        "\n",
        "    # Remove temporary columns\n",
        "    df_train = df_train[['gold_label', 'combined_sentences', 'attention_mask', 'token_type']]\n",
        "    df_dev = df_dev[['gold_label', 'combined_sentences', 'attention_mask', 'token_type']]\n",
        "    df_test = df_test[['gold_label', 'combined_sentences', 'attention_mask', 'token_type']]\n",
        "\n",
        "    return df_train,df_dev,df_test\n",
        "\n"
      ],
      "metadata": {
        "id": "_IIyw1hqX5h1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train_snli,df_dev_snli,df_test_snli = preprocess_df(df_train_snli,df_dev_snli,df_test_snli)\n",
        "df_train_multinli,df_dev_multinli,df_test_multinli = preprocess_df(df_train_multinli,df_dev_multinli,df_test_multinli)\n",
        "df_train_anli,df_dev_anli,df_test_anli = preprocess_df(df_train_anli,df_dev_anli,df_test_anli)"
      ],
      "metadata": {
        "id": "SVZAj44CZMuy"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows without label\n",
        "df_train_snli = df_train_snli[df_train_snli['gold_label'] != '-'] \n",
        "df_dev_snli = df_dev_snli[df_dev_snli['gold_label'] != '-'] \n",
        "df_test_snli = df_test_snli[df_test_snli['gold_label'] != '-'] \n",
        "\n",
        "df_train_multinli = df_train_multinli[df_train_multinli['gold_label'] != '-'] \n",
        "df_dev_multinli = df_dev_multinli[df_dev_multinli['gold_label'] != '-'] \n",
        "df_test_multinli = df_test_multinli[df_test_multinli['gold_label'] != '-'] \n",
        "\n",
        "df_train_anli = df_train_anli[df_train_anli['gold_label'] != '-'] \n",
        "df_dev_anli = df_dev_anli[df_dev_anli['gold_label'] != '-'] \n",
        "df_test_anli = df_test_anli[df_test_anli['gold_label'] != '-'] "
      ],
      "metadata": {
        "id": "4KbB6WdxIE7C"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save as csv\n",
        "df_train_snli.to_csv('snli_1.0/snli_1.0_train.csv', index=False)\n",
        "df_dev_snli.to_csv('snli_1.0/snli_1.0_dev.csv', index=False)\n",
        "df_test_snli.to_csv('snli_1.0/snli_1.0_test.csv', index=False)\n",
        "\n",
        "!mkdir multinli_1.0\n",
        "df_train_multinli.to_csv('multinli_1.0/multinli_1.0_train.csv', index=False)\n",
        "df_dev_multinli.to_csv('multinli_1.0/multinli_1.0_dev.csv', index=False)\n",
        "df_test_multinli.to_csv('multinli_1.0/multinli_1.0_test.csv', index=False)\n",
        "\n",
        "!mkdir anli_1.0\n",
        "df_train_anli.to_csv('anli_1.0/anli_1.0_train.csv', index=False)\n",
        "df_dev_anli.to_csv('anli_1.0/anli_1.0_dev.csv', index=False)\n",
        "df_test_anli.to_csv('anli_1.0/anli_1.0_test.csv', index=False)"
      ],
      "metadata": {
        "id": "CMvGxAZ7gv8A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_snli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = bert_tokenizer.convert_tokens_to_ids,pad_token = pad_id,unk_token = unk_id)\n",
        "labels_snli = torchtext.data.LabelField()\n",
        "attention_mask_snli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = lambda x : [int(y) for y in x], pad_token = pad_id)\n",
        "token_type_snli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = lambda x : [int(y) for y in x], pad_token = 1)\n",
        "\n",
        "seq_multinli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = bert_tokenizer.convert_tokens_to_ids,pad_token = pad_id,unk_token = unk_id)\n",
        "labels_multinli = torchtext.data.LabelField()\n",
        "attention_mask_multinli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = lambda x : [int(y) for y in x], pad_token = pad_id)\n",
        "token_type_multinli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = lambda x : [int(y) for y in x], pad_token = 1)\n",
        "\n",
        "seq_anli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = bert_tokenizer.convert_tokens_to_ids,pad_token = pad_id,unk_token = unk_id)\n",
        "labels_anli = torchtext.data.LabelField()\n",
        "attention_mask_anli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = lambda x : [int(y) for y in x], pad_token = pad_id)\n",
        "token_type_anli = torchtext.data.Field(batch_first = True,use_vocab = False,tokenize = lambda x : x.strip().split(\" \")[:254],preprocessing = lambda x : [int(y) for y in x], pad_token = 1)\n"
      ],
      "metadata": {
        "id": "hxpqAn7iJP4w"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fields_snli = [('label', labels_snli), ('sequence', seq_snli), ('attention_mask', attention_mask_snli), ('token_type', token_type_snli)]\n",
        "\n",
        "fields_multinli = [('label', labels_multinli), ('sequence', seq_multinli), ('attention_mask', attention_mask_multinli), ('token_type', token_type_multinli)]\n",
        "\n",
        "fields_anli = [('label', labels_anli), ('sequence', seq_anli), ('attention_mask', attention_mask_anli), ('token_type', token_type_anli)]\n",
        "\n",
        "train_data_snli, valid_data_snli, test_data_snli = torchtext.data.TabularDataset.splits(\n",
        "                                        path = 'snli_1.0',\n",
        "                                        train = 'snli_1.0_train.csv',\n",
        "                                        validation = 'snli_1.0_dev.csv',\n",
        "                                        test = 'snli_1.0_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields_snli,\n",
        "                                        skip_header = True)\n",
        "\n",
        "train_data_multinli, valid_data_multinli, test_data_multinli = torchtext.data.TabularDataset.splits(\n",
        "                                        path = 'multinli_1.0',\n",
        "                                        train = 'multinli_1.0_train.csv',\n",
        "                                        validation = 'multinli_1.0_dev.csv',\n",
        "                                        test = 'multinli_1.0_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields_multinli,\n",
        "                                        skip_header = True)\n",
        "\n",
        "train_data_anli, valid_data_anli, test_data_anli = torchtext.data.TabularDataset.splits(\n",
        "                                        path = 'anli_1.0',\n",
        "                                        train = 'anli_1.0_train.csv',\n",
        "                                        validation = 'anli_1.0_dev.csv',\n",
        "                                        test = 'anli_1.0_test.csv',\n",
        "                                        format = 'csv',\n",
        "                                        fields = fields_anli,\n",
        "                                        skip_header = True)\n",
        "\n",
        "labels_snli.build_vocab(train_data_snli)\n",
        "labels_multinli.build_vocab(train_data_multinli)\n",
        "labels_anli.build_vocab(train_data_anli)"
      ],
      "metadata": {
        "id": "YLWcII7lRQ58"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Parameters\n",
        "batch_size_snli = 16\n",
        "hidden_dim_snli = 512\n",
        "output_dim_snli = len(labels_snli.vocab)\n",
        "\n"
      ],
      "metadata": {
        "id": "mqXmCULuSsu9"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_multinli = 16\n",
        "hidden_dim_multinli = 512\n",
        "output_dim_multinli = len(labels_multinli.vocab)\n",
        "\n"
      ],
      "metadata": {
        "id": "_DDKbCo_MIUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_anli = 16\n",
        "hidden_dim_anli = 512\n",
        "output_dim_anli = len(labels_anli.vocab)"
      ],
      "metadata": {
        "id": "KlRL67WpmSyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Iterators\n",
        "train_iterator_snli, valid_iterator_snli, test_iterator_snli = torchtext.data.BucketIterator.splits(\n",
        "    (train_data_snli, valid_data_snli, test_data_snli), \n",
        "    batch_size = batch_size_snli,\n",
        "    sort_key = lambda x: len(x.sequence),\n",
        "    sort_within_batch = False, \n",
        "    device = device)\n",
        "\n"
      ],
      "metadata": {
        "id": "kylX4PXsU3wW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterator_multinli, valid_iterator_multinli, test_iterator_multinli = torchtext.data.BucketIterator.splits(\n",
        "    (train_data_multinli, valid_data_multinli, test_data_multinli), \n",
        "    batch_size = batch_size_multinli,\n",
        "    sort_key = lambda x: len(x.sequence),\n",
        "    sort_within_batch = False, \n",
        "    device = device)\n",
        "\n"
      ],
      "metadata": {
        "id": "KA5Q8rV_MG2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iterator_anli, valid_iterator_anli, test_iterator_anli = torchtext.data.BucketIterator.splits(\n",
        "    (train_data_anli, valid_data_anli, test_data_anli), \n",
        "    batch_size = batch_size_anli,\n",
        "    sort_key = lambda x: len(x.sequence),\n",
        "    sort_within_batch = False, \n",
        "    device = device)"
      ],
      "metadata": {
        "id": "_BelhGH8mRGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model\n",
        "class Bert_Model(nn.Module):\n",
        "  def __init__(self,output_dim):\n",
        "    super().__init__()\n",
        "    self.bert = bert\n",
        "    embedding_dim = bert.config.to_dict()['hidden_size']\n",
        "    self.out = nn.Linear(embedding_dim, output_dim)\n",
        "  def forward(self, seq, attention_mask, token_type):\n",
        "    embeddings = self.bert(input_ids = seq, attention_mask = attention_mask, token_type_ids= token_type)[1]\n",
        "    return self.out(embeddings)\n"
      ],
      "metadata": {
        "id": "8i1eA2cdTrKz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models, optimizers, loss functions\n",
        "#SNLI\n",
        "bert_model_snli = Bert_Model(output_dim_snli).to(device)\n",
        "optimizer_snli = AdamW(bert_model_snli.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
        "criterion_snli = nn.CrossEntropyLoss().to(device)\n"
      ],
      "metadata": {
        "id": "eQN-0bnnU94Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MultiNLI\n",
        "bert_model_multinli = Bert_Model(output_dim_multinli).to(device)\n",
        "optimizer_multinli = AdamW(bert_model_multinli.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
        "criterion_multinli = nn.CrossEntropyLoss().to(device)\n"
      ],
      "metadata": {
        "id": "30SmuITxmPRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ANLI\n",
        "bert_model_anli = Bert_Model(output_dim_anli).to(device)\n",
        "optimizer_anli = AdamW(bert_model_anli.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
        "criterion_anli = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "Zh-YEW0WMC8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reports\n",
        "def get_reports(y_pred,y_true,roc_want=False):\n",
        "  y_true = np.array([t.item() for t in y_true])\n",
        "  y_pred = np.array([np.argmax(t.cpu()) for t in y_pred])\n",
        "\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  print(\"Accuracy for model:\", accuracy)\n",
        "\n",
        "  print('------- CLASSIFICATION REPORT --------')\n",
        "  report = classification_report(y_true, y_pred)\n",
        "  print(report)\n",
        "  print('--------------------------------------')\n",
        "\n",
        "  #Confusion matrix\n",
        "  print('---------- CONFUSION MATRIX ----------')\n",
        "  cmat = confusion_matrix(y_true, y_pred)\n",
        "  print(cmat)\n",
        "  print('--------------------------------------')\n"
      ],
      "metadata": {
        "id": "JbAnKp5Jlajx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Helper to calculate accuracy\n",
        "def categorical_accuracy(preds, y):\n",
        "    max_preds = preds.argmax(dim = 1, keepdim = True)\n",
        "    correct = (max_preds.squeeze(1)==y).float()\n",
        "    return correct.sum() / len(y)"
      ],
      "metadata": {
        "id": "yU7HJQt2WWhq"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train model\n",
        "def train(bert_model,train_iterator,criterion,optimizer):\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  bert_model.train()\n",
        "  for batch in train_iterator:    \n",
        "    optimizer.zero_grad()\n",
        "    torch.cuda.empty_cache()\n",
        "    predictions = bert_model(batch.sequence, batch.attention_mask, batch.token_type)\n",
        "    loss = criterion(predictions, batch.label)\n",
        "    # print(predictions.shape)\n",
        "    acc = categorical_accuracy(predictions, batch.label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    # scheduler.step()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_acc += acc.item()\n",
        "      \n",
        "  return epoch_loss/len(train_iterator),epoch_acc/len(train_iterator)\n"
      ],
      "metadata": {
        "id": "X3XWjZ21VofE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate model\n",
        "\n",
        "def evaluate(bert_model,iterator,criterion):\n",
        "  all_pred = []\n",
        "  all_true = []\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  bert_model.eval()\n",
        "  with torch.no_grad():\n",
        "    for batch in iterator:\n",
        "      predictions = bert_model(batch.sequence, batch.attention_mask, batch.token_type)\n",
        "      loss = criterion(predictions, batch.label)\n",
        "      all_pred.extend(predictions)\n",
        "      all_true.extend(batch.label)\n",
        "      acc = categorical_accuracy(predictions, batch.label)\n",
        "      \n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += acc.item()\n",
        "  \n",
        "  return epoch_loss/len(iterator),epoch_acc/len(iterator),all_pred,all_true"
      ],
      "metadata": {
        "id": "FQGqwdw_WbNA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(bert_model,train_iterator,valid_iterator,optimizer,criterion,dataset_name,epochs=2):\n",
        "  \n",
        "  all_pred = []\n",
        "  all_true = []\n",
        "  for epoch in range(epochs):\n",
        "    start_time = time.time()\n",
        "     \n",
        "    train_loss, train_acc = train(bert_model,train_iterator,criterion,optimizer)\n",
        "    valid_loss, valid_acc,predictions,true_labels = evaluate(bert_model,valid_iterator,criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    torch.save(bert_model.state_dict(), 'bert_model_'+ dataset_name +'.pth')\n",
        "    \n",
        "    print('Epoch:',epoch+1,', Time taken:',str((end_time-start_time)/60))\n",
        "    print('Train Loss:',train_loss,', Train Acc:',train_acc*100)\n",
        "    print('Validation Loss:',valid_loss,', Validation Acc:',valid_acc*100)\n",
        "\n"
      ],
      "metadata": {
        "id": "zeftcIhNXDqE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bert_model_snli = Bert_Model(output_dim_snli).to(device)\n",
        "# bert_model_snli.load_state_dict(torch.load('bert_model.pth',map_location=torch.device('cpu')),)\n",
        "# optimizer_snli = AdamW(bert_model_snli.parameters(),lr=2e-5,eps=1e-6,correct_bias=False)\n",
        "# criterion_snli = nn.CrossEntropyLoss().to(device)"
      ],
      "metadata": {
        "id": "-gsY_boq5s2c"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(bert_model_snli,train_iterator_snli,valid_iterator_snli,optimizer_snli,criterion_snli,'snli',epochs=2)"
      ],
      "metadata": {
        "id": "vjfecGWkTmx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(bert_model_multinli,train_iterator_multinli,valid_iterator_multinli,optimizer_multinli,criterion_multinli,'multinli',epochs=4)"
      ],
      "metadata": {
        "id": "m5Kb0yOLTsNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(bert_model_anli,train_iterator_anli,valid_iterator_anli,optimizer_anli,criterion_anli,'anli',epochs=4)"
      ],
      "metadata": {
        "id": "oPDrv7DJmZGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_accuracy(bert_model,test_iterator,criterion,dataset_name):\n",
        "  bert_model.load_state_dict(torch.load('bert_model_'+ dataset_name +'.pth',map_location=torch.device('cpu')))\n",
        "  test_loss, test_acc,all_pred_test,all_true_test = evaluate(bert_model,test_iterator,criterion)\n",
        "  print('Test Loss:',test_loss,', Test Acc:',test_acc*100)\n",
        "  get_reports(all_pred_test,all_true_test,False)"
      ],
      "metadata": {
        "id": "m1lE7XS5RRME"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_test_accuracy(bert_model_snli,test_iterator_snli,criterion_snli,'snli')"
      ],
      "metadata": {
        "id": "zgtj-JjXVYhk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_test_accuracy(bert_model_multinli,test_iterator_multinli,criterion_multinli,'multinli')"
      ],
      "metadata": {
        "id": "uLDhn1PvVbAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_test_accuracy(bert_model_anli,test_iterator_anli,criterion_anli,'anli')"
      ],
      "metadata": {
        "id": "mizNoSkXrFCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_inference(premise, hypothesis, bert_model,labels):\n",
        "    torch.cuda.empty_cache()\n",
        "    bert_model.eval()    \n",
        "    premise = cls + ' ' + premise + ' ' + sep\n",
        "    hypothesis = hypothesis + ' ' + sep\n",
        "    \n",
        "    premise_tokens = bert_tokenizer.tokenize(premise)\n",
        "    hypothesis_tokens = bert_tokenizer.tokenize(hypothesis)\n",
        "   \n",
        "    premise_token_type = get_token_type(premise_tokens,0)\n",
        "    hypothesis_token_type = get_token_type(hypothesis_tokens,1)\n",
        "    \n",
        "    seq = premise_tokens + hypothesis_tokens\n",
        "    seq = bert_tokenizer.convert_tokens_to_ids(seq)\n",
        "    tokens_type = premise_token_type + hypothesis_token_type\n",
        "    attention_mask = get_token_type(seq,1)\n",
        "    \n",
        "    seq = torch.LongTensor(seq).unsqueeze(0).to(device)\n",
        "    tokens_type = torch.LongTensor(tokens_type).unsqueeze(0).to(device)\n",
        "    attention_mask = torch.LongTensor(attention_mask).unsqueeze(0).to(device)\n",
        "    \n",
        "    prediction = bert_model(seq, attention_mask, tokens_type)\n",
        "    prediction = prediction.argmax(dim=-1).item()\n",
        "    \n",
        "    return labels.vocab.itos[prediction]"
      ],
      "metadata": {
        "id": "AYG6hyOxV02u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "premise = 'a man sitting on a green bench.'\n",
        "hypothesis = 'a woman sitting on a green bench.'\n",
        "\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_snli))\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_multinli))\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_anli))"
      ],
      "metadata": {
        "id": "jYPG5xQOXOvk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84dd1c8d-4b8d-4237-f2dc-670ff2fe43f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contradiction\n",
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "premise = 'A black race car starts up in front of a crowd of people.'\n",
        "hypothesis = 'A man is driving down a lonely road.'\n",
        "\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_snli))\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_multinli))\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_anli))"
      ],
      "metadata": {
        "id": "qkWvjiOvXVEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdc9e04a-409e-4777-8ccb-a993d6e2edde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contradiction\n",
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "premise = 'A smiling costumed woman is holding an umbrella.'\n",
        "hypothesis = 'A happy woman in a fairy costume holds an umbrella.'\n",
        "\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_snli))\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_multinli))\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_anli))"
      ],
      "metadata": {
        "id": "khee9NI1XX23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a0a0a1e-370e-4dc9-ae1e-7a4c729c0286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "contradiction\n",
            "neutral\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "premise = 'A soccer game with multiple males playing.'\n",
        "hypothesis = 'Some men are playing a sport.'\n",
        "\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_snli))\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_multinli))\n",
        "print(predict_inference(premise, hypothesis,bert_model_snli,labels_anli))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f59O6fSXVUwo",
        "outputId": "0042648f-b649-4f71-ac39-3cc4cb424c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "entailment\n",
            "entailment\n"
          ]
        }
      ]
    }
  ]
}